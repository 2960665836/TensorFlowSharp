<Type Name="RMSProp" FullName="TensorFlow.RMSProp">
  <TypeSignature Language="C#" Value="public sealed class RMSProp : TensorFlow.AdaptiveOptimizer" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi sealed beforefieldinit RMSProp extends TensorFlow.AdaptiveOptimizer" />
  <AssemblyInfo>
    <AssemblyName>TensorFlowSharp</AssemblyName>
    <AssemblyVersion>1.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>TensorFlow.AdaptiveOptimizer</BaseTypeName>
  </Base>
  <Interfaces />
  <Docs>
    <summary>
            RMSProp: Adaptive stochastic gradient descent optimizer.
            </summary>
    <remarks>To be added.</remarks>
  </Docs>
  <Members>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public RMSProp (TensorFlow.TFGraph graph, float learningRate, float beta = 0.9, float decay = 0, float initialAccumulatorValue = 0.1, string operName = &quot;AdagradOptimizer&quot;);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class TensorFlow.TFGraph graph, float32 learningRate, float32 beta, float32 decay, float32 initialAccumulatorValue, string operName) cil managed" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="graph" Type="TensorFlow.TFGraph" />
        <Parameter Name="learningRate" Type="System.Single" />
        <Parameter Name="beta" Type="System.Single" />
        <Parameter Name="decay" Type="System.Single" />
        <Parameter Name="initialAccumulatorValue" Type="System.Single" />
        <Parameter Name="operName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="graph">The graph object.</param>
        <param name="learningRate">The learning rate for the SGD update.</param>
        <param name="beta">Factor to compute the moving average over square of gradients.</param>
        <param name="decay">Learning rate decay over each update.</param>
        <param name="initialAccumulatorValue">A floating point value. Starting value for the accumulators, must be positive.</param>
        <param name="operName">Name the optimizer. All the variable that are created in this class will be created under this scope.</param>
        <summary>
            Construct Adagrad optimizer.
            </summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="ApplyGradient">
      <MemberSignature Language="C#" Value="public override TensorFlow.TFOperation[] ApplyGradient (ValueTuple&lt;TensorFlow.TFOutput,TensorFlow.Variable&gt;[] gradientsAndVariables);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig virtual instance class TensorFlow.TFOperation[] ApplyGradient(valuetype System.ValueTuple`2&lt;valuetype TensorFlow.TFOutput, class TensorFlow.Variable&gt;[] gradientsAndVariables) cil managed" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>TensorFlow.TFOperation[]</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="gradientsAndVariables" Type="System.ValueTuple&lt;TensorFlow.TFOutput,TensorFlow.Variable&gt;[]">
          <Attributes>
            <Attribute>
              <AttributeName>System.Runtime.CompilerServices.TupleElementNames(Mono.Cecil.CustomAttributeArgument[])</AttributeName>
            </Attribute>
          </Attributes>
        </Parameter>
      </Parameters>
      <Docs>
        <param name="gradientsAndVariables">To be added.</param>
        <summary>To be added.</summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
        <inheritdoc />
      </Docs>
    </Member>
  </Members>
</Type>
